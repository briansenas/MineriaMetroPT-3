{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_csv(\"../data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intervalos anómalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "anomalies = [\n",
    "    (datetime.datetime(2020, 4, 12, 11, 50), datetime.datetime(2020, 4, 12, 23, 30)),\n",
    "    (datetime.datetime(2020, 4, 18, 0, 0), datetime.datetime(2020, 4, 18, 23, 59)),\n",
    "    (datetime.datetime(2020, 4, 19, 0, 0), datetime.datetime(2020, 4, 19, 1, 30)),\n",
    "    (datetime.datetime(2020, 4, 29, 3, 20), datetime.datetime(2020, 4, 29, 4, 0)),\n",
    "    (datetime.datetime(2020, 4, 29, 22, 0), datetime.datetime(2020, 4, 29, 22, 20)),\n",
    "    (datetime.datetime(2020, 5, 13, 14, 0), datetime.datetime(2020, 5, 13, 23, 59)),\n",
    "    (datetime.datetime(2020, 5, 18, 5, 0), datetime.datetime(2020, 5, 18, 5, 30)),\n",
    "    (datetime.datetime(2020, 5, 19, 10, 10), datetime.datetime(2020, 5, 19, 11, 0)),\n",
    "    (datetime.datetime(2020, 5, 19, 22, 10), datetime.datetime(2020, 5, 19, 23, 59)),\n",
    "    (datetime.datetime(2020, 5, 20, 0, 0), datetime.datetime(2020, 5, 20, 20, 0)),\n",
    "    (datetime.datetime(2020, 5, 23, 9, 50), datetime.datetime(2020, 5, 23, 10, 10)),\n",
    "    (datetime.datetime(2020, 5, 29, 23, 30), datetime.datetime(2020, 5, 29, 23, 59)),\n",
    "    (datetime.datetime(2020, 5, 30, 0, 0), datetime.datetime(2020, 5, 30, 6, 0)),\n",
    "    (datetime.datetime(2020, 6, 1, 15, 0), datetime.datetime(2020, 6, 1, 15, 40)),\n",
    "    (datetime.datetime(2020, 6, 3, 10, 0), datetime.datetime(2020, 6, 3, 11, 0)),\n",
    "    (datetime.datetime(2020, 6, 5, 10, 0), datetime.datetime(2020, 6, 5, 23, 59)),\n",
    "    (datetime.datetime(2020, 6, 6, 0, 0), datetime.datetime(2020, 6, 6, 23, 59)),\n",
    "    (datetime.datetime(2020, 6, 7, 0, 0), datetime.datetime(2020, 6, 7, 14, 30)),\n",
    "    (datetime.datetime(2020, 7, 8, 17, 30), datetime.datetime(2020, 7, 8, 19, 0)),\n",
    "    (datetime.datetime(2020, 7, 15, 14, 30), datetime.datetime(2020, 7, 15, 19, 0)),\n",
    "    (datetime.datetime(2020, 7, 17, 4, 30), datetime.datetime(2020, 7, 17, 5, 30)),\n",
    "]\n",
    "\n",
    "\n",
    "raros = [\n",
    "    #NO APARECE COMO ANOMALIA\n",
    "    (datetime.datetime(2020, 3, 6, 21, 42, 15), datetime.datetime(2020, 3, 6, 23, 14, 0)),\n",
    "    (datetime.datetime(2020, 3, 11, 5, 15, 10), datetime.datetime(2020, 3, 11, 6, 25, 0)),\n",
    "    (datetime.datetime(2020, 3, 12, 0, 15, 56), datetime.datetime(2020, 3, 12, 11, 59, 0)),\n",
    "    (datetime.datetime(2020, 3, 26, 4, 0, 20), datetime.datetime(2020, 3, 26, 5, 20, 0)),\n",
    "    (datetime.datetime(2020, 3, 27, 7, 12, 0), datetime.datetime(2020, 3, 27, 12, 1, 0)),\n",
    "    (datetime.datetime(2020, 4, 17, 8, 50, 28), datetime.datetime(2020, 4, 17, 23, 59, 0)),\n",
    "    (datetime.datetime(2020, 4, 25, 0, 7, 15), datetime.datetime(2020, 4, 25, 1, 10, 0)),\n",
    "    (datetime.datetime(2020, 5, 19, 1, 35, 28), datetime.datetime(2020, 5, 19, 2, 40, 0)),\n",
    "    (datetime.datetime(2020, 6, 12, 1, 41, 7), datetime.datetime(2020, 6, 12, 17, 6, 0)),\n",
    "    (datetime.datetime(2020, 7, 21, 13, 32, 48), datetime.datetime(2020, 7, 21, 22, 3, 0)),\n",
    "    (datetime.datetime(2020, 7, 22, 6, 40, 46), datetime.datetime(2020, 7, 22, 13, 10, 0)),\n",
    "    (datetime.datetime(2020, 7, 31, 0, 57, 33), datetime.datetime(2020, 7, 31, 2, 9, 0))\n",
    "]\n",
    "\n",
    "anomalies.extend(raros)\n",
    "\n",
    "# Correción de periodos temporales.\n",
    "final_anomalies = []\n",
    "for anomaly in anomalies: \n",
    "  new = list(anomaly)\n",
    "  if anomaly[1].minute == 59:\n",
    "    new[1] += datetime.timedelta(minutes=1)\n",
    "  final_anomalies.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df = df.with_columns(pl.col(\"timestamp\").str.to_datetime())\n",
    "df = df.with_row_index(\"rownr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies[1][1] + datetime.timedelta(seconds=59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definimos las anomalias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_anomaly(instance_date, anomalies: list[datetime]):\n",
    "    flag_anomaly = False\n",
    "    index = 0\n",
    "    while not flag_anomaly and index < len(anomalies):\n",
    "        t = anomalies[index]\n",
    "        if instance_date >= t[0] and instance_date <= t[1] + datetime.timedelta(seconds=59):\n",
    "            flag_anomaly = True\n",
    "        index += 1\n",
    "    return flag_anomaly\n",
    "\n",
    "\n",
    "df = df.select(pl.all(), pl.lit(0).alias(\"is_anomaly\"))\n",
    "for anomaly in final_anomalies:\n",
    "  df = df.with_columns((pl.col(\"is_anomaly\") + df[\"timestamp\"].is_between(anomaly[0], anomaly[1])))\n",
    "df = df.select(pl.exclude(\"is_anomaly\"), pl.col(\"is_anomaly\") >= 1)\n",
    "# df = df.select(pl.all(), pl.col(\"timestamp\").map_elements(lambda x: is_anomaly(x, anomalies), return_dtype=pl.Boolean).alias(\"is_anomaly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justificamos el cálculo de la ventana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transition states\n",
    "tdf = df.select(pl.all(), pl.col(\"is_anomaly\").shift(-1).alias(\"next_is_anomaly\"), pl.col(\"Motor_current\").gt(0.05).alias(\"motor_state\"))\n",
    "tdf = tdf.select(pl.all(), (pl.col(\"is_anomaly\") != pl.col(\"next_is_anomaly\")).alias(\"transition\"))\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create groups based on transition\n",
    "tdf = tdf.select(pl.all(), pl.col(\"transition\").cum_sum().alias(\"groups\"))\n",
    "tdf[\"groups\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = tdf.select(pl.all(), pl.col(\"groups\").shift(-1).alias(\"next_group\"), pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\"))\n",
    "# Filter a ON switch\n",
    "fdf = gdf.filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1)).with_columns(pl.col(\"timestamp\").shift(-1).alias(\"next_timestamp\"))\n",
    "# Filter out different time groups\n",
    "fdf = fdf.filter(pl.col(\"groups\") == pl.col(\"next_group\"))\n",
    "# Calculate the duration\n",
    "fdf = fdf.with_columns((pl.col(\"next_timestamp\") - pl.col(\"timestamp\")).alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "print(fdf[\"duration\"].dt.total_seconds().max(), fdf[\"duration\"].dt.total_seconds().min(), fdf[\"duration\"].dt.total_seconds().mean(), fdf[\"duration\"].dt.total_seconds().mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.filter(pl.col(\"duration\").eq(pl.col(\"duration\").max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "data_anomaly = df.filter(pl.col(\"timestamp\").is_between(\n",
    "datetime.datetime(2020,6,5,00,00,00), datetime.datetime(2020,6,8,15,00,00)\n",
    "))\n",
    "px.line(data_anomaly, x=\"timestamp\", y=[\"TP3\", \"Motor_current\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_anomaly.select(pl.col(\"is_anomaly\"), pl.col(\"is_anomaly\").shift(-1).alias(\"next_is_anomaly\"), pl.col(\"timestamp\"), pl.col(\"timestamp\").shift(-1).alias(\"next_timestamp\")).filter(~pl.col(\"is_anomaly\").eq(pl.col(\"next_is_anomaly\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para cada grupo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = tdf.select(pl.all(), pl.col(\"groups\").shift(-1).alias(\"next_group\"), pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\"))\n",
    "# Filter a ON switch\n",
    "durations = []\n",
    "for label, group in gdf.filter(pl.col(\"groups\") == pl.col(\"next_group\")).group_by(\"groups\"):\n",
    "  tmp = group.filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1)).with_columns(pl.col(\"timestamp\").shift(-1).alias(\"next_timestamp\"))\n",
    "  tmp = tmp.with_columns((pl.col(\"next_timestamp\") - pl.col(\"timestamp\")).alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "  durations += tmp[\"duration\"].to_list()\n",
    "durations = [x.total_seconds() for x in durations]\n",
    "print(max(durations), min(durations), sum(durations) / len(durations), durations[len(durations)//2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para el primer tramo no-anómalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df.filter(pl.col(\"timestamp\").lt(anomalies[0][0]))\n",
    "subdf = subdf.with_columns(pl.col(\"Motor_current\").lt(0.05).alias(\"motor_state\"))\n",
    "subdf.with_columns(pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\")).filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1))[\"timestamp\"].diff().dt.total_seconds().mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para cada grupo evitando saltos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "subdf = df.filter(pl.col(\"timestamp\").lt(final_anomalies[0][0]))\n",
    "subdf = subdf.with_columns(pl.col(\"Motor_current\").lt(0.05).alias(\"motor_state\"))\n",
    "subdf = subdf.with_columns(pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\")).filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1))\n",
    "subdf = subdf.with_columns(pl.col(\"timestamp\").diff().dt.total_seconds().alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "print(len(durations), len(subdf))\n",
    "durations += subdf[\"duration\"].to_list()\n",
    "for anomaly, next_anomaly, next_next_anomaly in zip(final_anomalies, final_anomalies[1:], final_anomalies[2:]):\n",
    "  subdf = df.filter(\n",
    "    (pl.col(\"timestamp\").lt(next_anomaly[0]) & (pl.col(\"timestamp\").gt(anomaly[1])))\n",
    "  )\n",
    "  subdf = subdf.with_columns(pl.col(\"Motor_current\").lt(0.05).alias(\"motor_state\"))\n",
    "  subdf = subdf.with_columns(pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\")).filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1))\n",
    "  subdf = subdf.with_columns(pl.col(\"timestamp\").diff().dt.total_seconds().alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "  print(len(durations), len(subdf))\n",
    "  durations += subdf[\"duration\"].to_list()\n",
    "  subdf = df.filter(\n",
    "    (pl.col(\"timestamp\").gt(next_anomaly[1]) & (pl.col(\"timestamp\").lt(next_next_anomaly[0])))\n",
    "  )\n",
    "  subdf = subdf.with_columns(pl.col(\"Motor_current\").lt(0.05).alias(\"motor_state\"))\n",
    "  subdf = subdf.with_columns(pl.col(\"motor_state\").shift(-1).alias(\"next_motor_state\")).filter((pl.col(\"motor_state\") == 0) & (pl.col(\"next_motor_state\") == 1))\n",
    "  subdf = subdf.with_columns(pl.col(\"timestamp\").diff().dt.total_seconds().alias(\"duration\")).filter(pl.col(\"duration\").is_not_null())\n",
    "  print(len(durations), len(subdf))\n",
    "  durations += subdf[\"duration\"].to_list()\n",
    "print(max(durations), min(durations), sum(durations) / len(durations), sorted(durations)[len(durations)//2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col(\"timestamp\").diff().cum_sum().alias(\"diff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col(\"timestamp\").diff().cum_sum().alias(\"diff\"))\n",
    "df = df.with_columns((pl.col(\"diff\").dt.total_seconds() // (1249 * 2)).alias(\"id\"))\n",
    "df = df.with_columns(pl.coalesce(pl.col(\"id\").cast(pl.UInt64), 0).alias(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.group_by(\"id\", maintain_order=True).len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "start = 51000\n",
    "end = 51000 + 10000\n",
    "px.line(df[start : end], x=\"timestamp\", y=\"TP3\", color=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intentamos rellenar valores pérdidos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "start = 51000\n",
    "end = 51000 + 10000\n",
    "test = pd.read_csv(\"../data/data.csv\")\n",
    "test = test.assign(TP3L = test.TP3.interpolate(method=\"linear\"))\n",
    "px.line(test[start : end], x=\"timestamp\", y=\"TP3L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendríamos que imputar solamente aquellos tramos cuya distancia fuera casi insignificante (algunos minutos). \n",
    "Ya que si observamos el Feb-9T00:00 a Feb-9T06:00 hemos introducido una línea similar a lo que seŕia una anomalía por el tamaño del salto temporal.\n",
    "\n",
    "Otra opción sería investigar introducir segmentos cíclicos hasta que la diferencia fuera manejable. No obstante, esto sería difícil de argumentar y confuso para los posibles segmentos anómalos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observar valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pl.Config(tbl_rows=1000):\n",
    "  print(df.filter(pl.col(\"timestamp\").gt(datetime.datetime(2020,2, 1, 19, 40, 00)) & pl.col(\"timestamp\").lt(datetime.datetime(2020, 2, 2, 6, 00, 00))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(pl.col(\"timestamp\").diff().dt.total_seconds().gt(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificar valores raros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df.filter(pl.col(\"timestamp\").is_between(*raros[5])), x=\"timestamp\", y=\"TP3\", color=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisar datos procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "\n",
    "df = pl.read_csv(\"../data/data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiempo_medio_ciclo = 1260\n",
    "df.filter(pl.col(\"timestamp\").str.to_datetime().diff().dt.total_seconds() > tiempo_medio_ciclo * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "tiempo_medio_ciclo = 1260\n",
    "df = pl.read_csv(\"../data/data.csv\")\n",
    "# Str timestamp to datetime object\n",
    "df = df.select(pl.exclude(\"timestamp\"), pl.col(\"timestamp\").str.to_datetime())\n",
    "# Calculate distance from next read\n",
    "df = df.with_columns(pl.col(\"timestamp\").diff().cum_sum().alias(\"diff\"))\n",
    "# Divide by mean cicle\n",
    "df = df.with_columns((pl.col(\"diff\").dt.total_seconds() // (tiempo_medio_ciclo * 2)).alias(\"id\"))\n",
    "# Count how many different samples are in each sliding sample \n",
    "df = df.join(df.group_by(\"id\").len(\"count\"), on=\"id\", how=\"inner\")\n",
    "# Filter those windows that have less than half points\n",
    "df = df.filter(pl.col(\"count\") >= (tiempo_medio_ciclo)/ (10))\n",
    "# Cast the ID column \n",
    "df = df.with_columns(pl.col(\"id\").cast(pl.UInt64).alias(\"id\"))\n",
    "# Asignamos la etiqueta\n",
    "df = df.select(pl.all(), pl.lit(0).alias(\"is_anomaly\"))\n",
    "for anomaly in final_anomalies:\n",
    "  df = df.with_columns((pl.col(\"is_anomaly\") + df[\"timestamp\"].is_between(anomaly[0], anomaly[1])))\n",
    "df = df.select(pl.exclude(\"is_anomaly\"), pl.col(\"is_anomaly\") >= 1)\n",
    "# Visualize a sample and the group length\n",
    "df.group_by(\"id\").first().with_columns(pl.col(\"count\").min().alias(\"smallest_window\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos las variables que vamos a utilizar.\n",
    "a = ['TP2',\n",
    "     'TP3',\n",
    "     'H1',\n",
    "     'DV_pressure',\n",
    "     'Reservoirs',\n",
    "     'Oil_temperature',\n",
    "     'Motor_current',\n",
    "     'COMP',\n",
    "     'DV_eletric',\n",
    "     'Towers',\n",
    "     'MPG',\n",
    "     'LPS',\n",
    "     'Pressure_switch',\n",
    "     'Oil_level',\n",
    "     'Caudal_impulses',\n",
    "]\n",
    "\n",
    "# Ahora obtenemos los diferentes ids que vamos a reducir (ventanas)\n",
    "df_X = df.select(\"id\").group_by(\"id\", maintain_order=True).first()\n",
    "\n",
    "# Calculamos las características\n",
    "for c in a:\n",
    "  df_aux = df.group_by(\"id\", maintain_order=True).agg([\n",
    "      pl.col(f\"{c}\").mean().alias(f\"{c}_mean\"),\n",
    "      pl.col(f\"{c}\").max().alias(f\"{c}_max\"),\n",
    "      pl.col(f\"{c}\").min().alias(f\"{c}_min\"),\n",
    "      pl.col(f\"{c}\").median().alias(f\"{c}_median\"),\n",
    "      pl.col(f\"{c}\").var().alias(f\"{c}_var\"),\n",
    "  ])\n",
    "\n",
    "  df_X = df_X.join(df_aux, on=\"id\", how=\"left\")\n",
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetamos según mayoría en la ventana.\n",
    "labels = df.select([\"id\", \"is_anomaly\"]).group_by(pl.all()).len(\"count\")\n",
    "labels = labels.select(pl.all(), pl.col(\"count\").max().over(\"id\").alias(\"democracy\"))\n",
    "labels = labels.filter(pl.col(\"count\").eq(pl.col(\"democracy\"))).select([\"id\", \"is_anomaly\"])\n",
    "\n",
    "# Etiquetamos las anomalías\n",
    "df_X = df_X.join(labels, how=\"inner\", on=\"id\")\n",
    "df_X = df_X.drop(\"id\").with_row_index(\"id\")\n",
    "df_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 140421\n",
    "#Separamos anomalias de no anomalias\n",
    "df_no_anomaly = df_X.filter(pl.col(\"is_anomaly\") == False)\n",
    "df_anomaly = df_X.filter(pl.col(\"is_anomaly\") == True)\n",
    "\n",
    "#Agrupamos las que pertenecen a una misma anomalia\n",
    "df_anomaly = df_anomaly.sort(\"id\").with_columns(pl.col(\"id\").shift(1).fill_null(0).alias(\"last_id\"))\n",
    "\n",
    "df_anomaly = df_anomaly.with_columns(((pl.col(\"id\") - pl.col(\"last_id\")) >= 2).alias(\"group\"))\n",
    "\n",
    "df_anomaly = df_anomaly.with_columns(pl.col(\"group\").cum_sum())\n",
    "df_anomaly = df_anomaly.join(df_anomaly.select(\"id\", \"group\").group_by(\"group\", maintain_order=True).len(name=\"count\"),\n",
    "                             on=\"group\", how=\"inner\")\n",
    "\n",
    "#Creamos grupos de los grupos con un mismo tamaño para un k prefijado\n",
    "k = 9\n",
    "test_anomaly_size = int(df_anomaly.shape[0] / k)\n",
    "df_groups = df_anomaly.group_by(\"group\", maintain_order=True).first().select(\"group\", \"count\")\n",
    "df_groups = df_groups.sample(fraction=1, seed=seed, shuffle=True)\n",
    "\n",
    "groups = []\n",
    "for i in df_groups.to_dicts():\n",
    "    index = 0\n",
    "    flag_assigned = False\n",
    "    while index < len(groups) and not flag_assigned:\n",
    "        if groups[index][\"count\"] + i[\"count\"] < test_anomaly_size:\n",
    "            groups[index][\"group\"].append(i[\"group\"])\n",
    "            groups[index][\"count\"] += i[\"count\"]\n",
    "            flag_assigned = True\n",
    "            index = 0\n",
    "        else:\n",
    "            index += 1\n",
    "    if not flag_assigned and len(groups) >= index:\n",
    "        if len(groups) < k:\n",
    "            groups.append({\"count\": i[\"count\"], \"group\": [i[\"group\"]]})\n",
    "        else:\n",
    "            min_size = min([a[\"count\"] for a in groups])\n",
    "            for index in range(0, k):\n",
    "                if groups[index][\"count\"] == min_size:\n",
    "                    groups[index][\"group\"].append(i[\"group\"])\n",
    "                    groups[index][\"count\"] += i[\"count\"]\n",
    "                    flag_assigned = True\n",
    "                    index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 140421\n",
    "#Repetimos el proceso para las no anomalias\n",
    "df_no_anomaly = df_no_anomaly.sort(\"id\").with_columns(pl.col(\"id\").shift(1).fill_null(0).alias(\"last_id\"))\n",
    "\n",
    "df_no_anomaly = df_no_anomaly.with_columns(((pl.col(\"id\") - pl.col(\"last_id\")) >= 2).alias(\"group\"))\n",
    "\n",
    "df_no_anomaly = df_no_anomaly.with_columns(pl.col(\"group\").cum_sum())\n",
    "df_no_anomaly = df_no_anomaly.join(df_no_anomaly.select(\"id\", \"group\").group_by(\"group\").len(name=\"count\"),\n",
    "                                   on=\"group\", how=\"inner\")\n",
    "\n",
    "#Creamos grupos de los grupos con un mismo tamaño para un k prefijado\n",
    "test_anomaly_size = int(df_no_anomaly.shape[0] / k)\n",
    "df_groups_no_anomaly = df_no_anomaly.group_by(\"group\", maintain_order=True).first().select(\"group\", \"count\")\n",
    "df_groups_no_anomaly = df_groups_no_anomaly.sample(fraction=1, seed=seed, shuffle=True)\n",
    "\n",
    "groups_no_anomaly = []\n",
    "for i in df_groups_no_anomaly.to_dicts():\n",
    "    index = 0\n",
    "    flag_assigned = False\n",
    "    while index < len(groups_no_anomaly) and not flag_assigned:\n",
    "        if groups_no_anomaly[index][\"count\"] + i[\"count\"] < test_anomaly_size:\n",
    "            groups_no_anomaly[index][\"group\"].append(i[\"group\"])\n",
    "            groups_no_anomaly[index][\"count\"] += i[\"count\"]\n",
    "            flag_assigned = True\n",
    "            index = 0\n",
    "        else:\n",
    "            index += 1\n",
    "    if not flag_assigned and len(groups_no_anomaly) >= index:\n",
    "        if len(groups_no_anomaly) < k:\n",
    "            groups_no_anomaly.append({\"count\": i[\"count\"], \"group\": [i[\"group\"]]})\n",
    "        else:\n",
    "            min_size = min([a[\"count\"] for a in groups_no_anomaly])\n",
    "            for index in range(0, k):\n",
    "                if groups_no_anomaly[index][\"count\"] == min_size:\n",
    "                    groups_no_anomaly[index][\"group\"].append(i[\"group\"])\n",
    "                    groups_no_anomaly[index][\"count\"] += i[\"count\"]\n",
    "                    flag_assigned = True\n",
    "                    index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0\n",
    "df_X = df_X.with_columns(pl.lit(None).alias(\"split_where_test\"))\n",
    "for a, b in zip(groups, groups_no_anomaly):\n",
    "    split_ids = df_anomaly.filter(pl.col(\"group\").is_in(a[\"group\"]))[\"id\"].to_list() + \\\n",
    "                df_no_anomaly.filter(pl.col(\"group\").is_in(b[\"group\"]))[\"id\"].to_list()\n",
    "    df_X = df_X.with_columns(\n",
    "        pl.when((pl.col(\"id\").is_in(split_ids))).then(split).otherwise(pl.col(\"split_where_test\")).alias(\"split_where_test\"))\n",
    "    split += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X.group_by(\"split_where_test\", \"is_anomaly\").len(name=\"count\").sort(\"split_where_test\", \"is_anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_X.filter(pl.col(\"split_where_test\").is_in([1,8]))\n",
    "df_test = df_test.select(pl.exclude([\"id\",\"split_where_test\"]))\n",
    "\n",
    "\n",
    "df_training = df_X.filter(~pl.col(\"split_where_test\").is_in([1,8]))\n",
    "df_training = df_training.with_columns(\n",
    "                                       pl.when(pl.col(\"split_where_test\").is_in([0,7]))\n",
    "                                       .then(0)\n",
    "                                       .when(pl.col(\"split_where_test\").is_in([2,6]))\n",
    "                                       .then(1)\n",
    "                                       .when(pl.col(\"split_where_test\").is_in([5,3]))\n",
    "                                       .then(2)\n",
    "                                       .when(pl.col(\"split_where_test\").is_in([4]))\n",
    "                                       .then(3).alias(\"split_where_test_2\"))\n",
    "df_training = df_training.select(pl.exclude([\"id\",\"split_where_test\"]))\n",
    "df_training = df_training.rename({\"split_where_test_2\": \"fold\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.group_by(\"fold\", \"is_anomaly\").len(name=\"count\").sort(\"fold\", \"is_anomaly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for a in groups:\n",
    "    if index not in [1,8,4]:\n",
    "        print(\"index: \",index ,\"count: \",a.get(\"count\"))\n",
    "    index += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.write_csv(\"../data/train.csv\")\n",
    "df_test.write_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar modelo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(140421)\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Set up cross-validation using the 'folds' column\n",
    "folds = train_data[fold_column].unique()\n",
    "\n",
    "cv = GridSearchCV(\n",
    "  estimator = GaussianNB(), \n",
    "  param_grid={\n",
    "    \"var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(cv.cv_results_))\n",
    "print(cv.best_estimator_, cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = cv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(cv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance and Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(140421)\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "folds = train_data[fold_column].unique()\n",
    "model = Pipeline([\n",
    "        ('sampling', CondensedNearestNeighbour(random_state=seed)),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "rcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb_var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "rcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(rcv.cv_results_))\n",
    "print(rcv.best_estimator_, rcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = rcv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(rcv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(140421)\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "folds = train_data[fold_column].unique()\n",
    "model = Pipeline([\n",
    "        ('sampling', SMOTETomek(random_state=seed)),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "rcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb__var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "rcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(rcv.cv_results_))\n",
    "print(rcv.best_estimator_, rcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = rcv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(rcv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRMR = [ \n",
    "  \"Towers_mean\", \n",
    "  \"DV_eletric_var\",\n",
    "  \"DV_pressure_median\",\n",
    "  \"TP2_max\",\n",
    "  \"Towers_median\",        \n",
    "  \"Reservoirs_max\",\n",
    "  \"DV_eletric_min\",       \n",
    "  \"Pressure_switch_min\", \n",
    "  \"Motor_current_var\",\n",
    "  \"Towers_var\",\n",
    "  \"LPS_min\",\n",
    "  \"DV_pressure_min\",\n",
    "  \"Towers_min\",\n",
    "  \"TP2_median\", \n",
    "  \"Oil_level_max\",\n",
    "  \"Reservoirs_mean\",\n",
    "  \"DV_pressure_mean\",\n",
    "  \"Oil_temperature_min\",\n",
    "  \"Oil_temperature_var\",  \n",
    "  \"DV_eletric_median\",\n",
    "  \"Caudal_impulses_max\",\n",
    "  \"DV_pressure_var\", \n",
    "  \"LPS_max\"              ,\n",
    "  \"H1_max\",\n",
    "  \"Motor_current_max\",\n",
    "  \"Motor_current_median\",\n",
    "  \"COMP_min\",\n",
    "  # \"TP2_var\",\n",
    "  # \"DV_eletric_mean\",      \n",
    "  # \"TP3_var\",\n",
    "  # \"Reservoirs_median\",\n",
    "  # \"H1_median\",\n",
    "]\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = MRMR\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Set up cross-validation using the 'folds' column\n",
    "folds = train_data[fold_column].unique()\n",
    "\n",
    "mcv = GridSearchCV(\n",
    "  estimator = GaussianNB(), \n",
    "  param_grid={\n",
    "    \"var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "mcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(mcv.cv_results_))\n",
    "print(mcv.best_estimator_, mcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = mcv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(mcv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHI-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Set up cross-validation using the 'folds' column\n",
    "folds = train_data[fold_column].unique()\n",
    "x_train = MinMaxScaler().fit_transform(X_train) \n",
    "chi_scores = chi2(x_train, y_train)\n",
    "p_values = pd.Series(chi_scores[1],index = X_train.columns)\n",
    "p_values.sort_values(ascending = False , inplace = True)\n",
    "p_values.plot.bar()\n",
    "plt.show()\n",
    "\n",
    "indexes = chi_scores[1] <= 0.05\n",
    "columns = X_train.columns\n",
    "print(columns[indexes])\n",
    "\n",
    "X_train = X_train.loc[:, indexes]\n",
    "\n",
    "ccv = GridSearchCV(\n",
    "  estimator = GaussianNB(), \n",
    "  param_grid={\n",
    "    \"var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "ccv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(ccv.cv_results_))\n",
    "print(ccv.best_estimator_, ccv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = ccv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(ccv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN + ChiSquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "# feature_columns = MRMR\n",
    "feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Set up cross-validation using the 'folds' column\n",
    "folds = train_data[fold_column].unique()\n",
    "x_train = MinMaxScaler().fit_transform(X_train) \n",
    "chi_scores = chi2(x_train, y_train)\n",
    "\n",
    "indexes = chi_scores[1] <= 0.05\n",
    "columns = X_train.columns\n",
    "print(columns[indexes])\n",
    "\n",
    "\n",
    "X_train = X_train.loc[:, indexes]\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', CondensedNearestNeighbour()),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "frcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb__var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "frcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(frcv.cv_results_))\n",
    "print(frcv.best_estimator_, frcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = frcv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(frcv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + MRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = MRMR\n",
    "# feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', CondensedNearestNeighbour()),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "frcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb__var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "frcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(frcv.cv_results_))\n",
    "print(frcv.best_estimator_, frcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = frcv.best_estimator_.predict(X_train[MRMR])\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(frcv.best_estimator_, X_train[MRMR], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTELinks + MRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "train_data.reset_index()\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "test_data.reset_index()\n",
    "\n",
    "target_column = 'is_anomaly'\n",
    "fold_column = 'fold'\n",
    "feature_columns = MRMR\n",
    "# feature_columns = [col for col in train_data.columns if col not in [target_column, fold_column]]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "\n",
    "def get_cv_iterable(\n",
    "  folds: list,\n",
    "  fold_column: str,\n",
    "  train: pd.DataFrame,\n",
    "): \n",
    "  for fold in folds:\n",
    "    test_indexes = train[train[fold_column] == fold].index\n",
    "    train_indexes = train[train[fold_column] != fold].index\n",
    "    yield (train_indexes, test_indexes)\n",
    "\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', SMOTETomek(random_state=seed)),\n",
    "        ('nb', GaussianNB())\n",
    "    ])\n",
    "\n",
    "frcv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid={\n",
    "    \"nb__var_smoothing\": np.linspace(1e-11, 1e-7, 100)\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "frcv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(frcv.cv_results_))\n",
    "print(frcv.best_estimator_, frcv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = frcv.best_estimator_.predict(X_train[MRMR])\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(frcv.best_estimator_, X_train[MRMR], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar el modelo de Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "seed = 140421\n",
    "# Utilizamos una versión básica de stacking\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=seed)),\n",
    "    ('svc', SVC(random_state=seed)),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "# Con un clasificador final lineal\n",
    "stack = StackingClassifier(\n",
    "  estimators=estimators,\n",
    "  final_estimator=LogisticRegression()\n",
    ")\n",
    "# Hacemos un GridSearch\n",
    "scv = GridSearchCV(\n",
    "  estimator = stack, \n",
    "  param_grid = {\n",
    "    # Decision Tree \n",
    "    'dt__max_depth': [None, 10],  # Unlimited or moderate depth\n",
    "    'dt__criterion': ['gini', 'entropy'],  # Gini or Entropy\n",
    "\n",
    "    # SVC\n",
    "    'svc__C': [0.1, 1, 10],  \n",
    "\n",
    "    # Naive Bayes\n",
    "    # 'nb__solver': [1e-9, 1e-8],  \n",
    "\n",
    "    # Logistic Regression\n",
    "    'final_estimator__penalty': [None, 'l2'], # Can focus on 1 model or be more moderate\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "scv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(scv.cv_results_))\n",
    "print(scv.best_estimator_, scv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = scv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(scv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients (assuming binary classification for simplicity)\n",
    "coefficients = scv.best_estimator_.final_estimator_.coef_.flatten()  # Flatten if multi-dimensional\n",
    "abs_coefficients = np.abs(coefficients)  # Get absolute values of coefficients\n",
    "total_weight = np.sum(abs_coefficients)  # Sum of absolute weights\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "data = {\n",
    "    \"Model Name\": [name for name, _ in stack.estimators],\n",
    "    \"Absolute Weight\": coefficients,\n",
    "    \"Relative Weight (%)\": (abs_coefficients / total_weight) * 100\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking + SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "seed = 140421\n",
    "# Utilizamos una versión básica de stacking\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=seed)),\n",
    "    ('svc', SVC(random_state=seed)),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "# Con un clasificador final lineal\n",
    "stack = StackingClassifier(\n",
    "  estimators=estimators,\n",
    "  final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "model = Pipeline([\n",
    "        ('sampling', SMOTETomek(random_state=seed)),\n",
    "        ('stack', stack)\n",
    "    ])\n",
    "# Hacemos un GridSearch\n",
    "scv = GridSearchCV(\n",
    "  estimator = model, \n",
    "  param_grid = {\n",
    "    # Decision Tree \n",
    "    'stack__dt__max_depth': [None, 10],  # Unlimited or moderate depth\n",
    "    'stack__dt__criterion': ['gini', 'entropy'],  # Gini or Entropy\n",
    "\n",
    "    # SVC\n",
    "    'stack__svc__C': [0.1, 1, 10],  \n",
    "\n",
    "    # Naive Bayes\n",
    "    # 'nb__solver': [1e-9, 1e-8],  \n",
    "\n",
    "    # Logistic Regression\n",
    "    'stack__final_estimator__penalty': [None, 'l2'], # Can focus on 1 model or be more moderate\n",
    "  },\n",
    "  cv = get_cv_iterable(folds, fold_column, train_data),\n",
    "  scoring=[\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\", \n",
    "    \"accuracy\"\n",
    "  ],\n",
    "  refit=\"f1\",\n",
    ")\n",
    "\n",
    "scv.fit(X_train, y_train)\n",
    "with pd.option_context(\"display.max_columns\", 33):\n",
    "  display(pd.DataFrame(scv.cv_results_))\n",
    "print(scv.best_estimator_, scv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred_train = scv.best_estimator_.predict(X_train)\n",
    "print(pd.DataFrame(classification_report(y_train, ypred_train, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(scv.best_estimator_, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients (assuming binary classification for simplicity)\n",
    "coefficients = scv.best_estimator_.named_steps[\"stack\"].final_estimator_.coef_.flatten() # Flatten if multi-dimensional\n",
    "abs_coefficients = np.abs(coefficients)  # Get absolute values of coefficients\n",
    "total_weight = np.sum(abs_coefficients)  # Sum of absolute weights\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "data = {\n",
    "    \"Model Name\": [name for name, _ in stack.estimators],\n",
    "    \"Absolute Weight\": coefficients,\n",
    "    \"Relative Weight (%)\": (abs_coefficients / total_weight) * 100\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicaciones operaciones sobre test con los mejores modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred = cv.best_estimator_.predict(X_test)\n",
    "print(pd.DataFrame(classification_report(y_test, ypred, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(cv.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ypred = scv.best_estimator_.predict(X_test)\n",
    "print(pd.DataFrame(classification_report(y_test, ypred, output_dict=True)))\n",
    "_ = ConfusionMatrixDisplay.from_estimator(scv.best_estimator_, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
